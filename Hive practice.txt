Date 17/Jul/2018 7AM



Order of Execution:

FROM
where
group by
having
SELECT
order by
limit


[cloudera@quickstart ~]$ cat > prof1
101,Ravi,30,btech#mtech#phd,hyd
102,Rani,25,bsc#msc#mtech,del
^C

hive> create database practice;
OK
Time taken: 0.503 seconds
hive> use practice;
OK
Time taken: 0.056 seconds

Explode----> flattens array elements into many rows

explode(['a','b','c'])-> 
a
b
c

==============================

3 types of functions in Hive:

1) UDF(user defined functions)
2) UDAF(user defined aggregated func)
3) UDTF (user def table generated func)

ex for udf: substr(), length,size()

for each row 1 value is returned

ex for UDAF: sum(),count(),avg(),max()

for entire column or group these func return single value

udtf ex: explode(),json_tuple()

explode([1,2,3])
1
2
3

these return tables in the form of row or columsn or rows and columns

[cloudera@quickstart ~]$ cat > sales
c101,10#20#100
c102,100#50
c103,300#500#800#100
c101,600#400
^C
[cloudera@quickstart ~]$  cat sales
c101,10#20#100
c102,100#50
c103,300#500#800#100
c101,600#400
login as: cloudera
cloudera@192.168.216.129's password:
Last login: Sun Jul 15 20:13:17 2018 from 192.168.216.1
[cloudera@quickstart ~]$ hive

Logging initialized using configuration in file:/etc/hive/conf.disroperties
WARNING: Hive CLI is deprecated and migration to Beeline is recomm
hive> create database practice;
OK
Time taken: 0.503 seconds
hive> use practice;
OK
Time taken: 0.056 seconds
hive> create table trans(cid string, price array<int>)
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '#';
OK
Time taken: 0.105 seconds
hive> load data local inpath 'sales' into trans;
FAILED: ParseException line 1:36 missing TABLE at 'trans' near '<EOF>'
hive> load data local inpath 'sales' into table trans;
Loading data to table practice.trans
Table practice.trans stats: [numFiles=1, totalSize=61]
OK
Time taken: 0.373 seconds
hive> select * from trans;
OK
c101    [10,20,100]
c102    [100,50]
c103    [300,500,800,100]
c101    [600,400]
Time taken: 0.074 seconds, Fetched: 4 row(s)
hive> select explode(price) as pr from trans;
OK
10
20
100
100
50
300
500
800
100
600
400
Time taken: 0.077 seconds, Fetched: 11 row(s)
hive> select cid, mypr from trans
    > lateral view explode(price) p as mypr;
OK
c101    10
c101    20
c101    100
c102    100
c102    50
c103    300
c103    500
c103    800
c103    100
c101    600
c101    400
Time taken: 0.131 seconds, Fetched: 11 row(s)
hive> create table strans(cid string, price int);
OK
Time taken: 0.09 seconds
hive> insert into table strans
    > select cid, mypr from trans
    > lateral view explode(price) p as mypr;
Query ID = cloudera_20180716192424_3a25f7f1-9976-4b4d-9076-12631c64de31
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0005, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 19:24:46,972 Stage-1 map = 0%,  reduce = 0%
2018-07-16 19:24:54,642 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.2 sec
MapReduce Total cumulative CPU time: 1 seconds 200 msec
Ended Job = job_1531648645163_0005
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice.db/strans/.hive-staging_hive_2018-07-16_19-24-37_898_1728537861197208337-1/-ext-10000
Loading data to table practice.strans
Table practice.strans stats: [numFiles=1, numRows=11, totalSize=96, rawDataSize=85]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.2 sec   HDFS Read: 4961 HDFS Write: 167 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 200 msec
OK
Time taken: 18.137 seconds
hive> select * from strans;
OK
c101    10
c101    20
c101    100
c102    100
c102    50
c103    300
c103    500
c103    800
c103    100
c101    600
c101    400
Time taken: 0.086 seconds, Fetched: 11 row(s)
hive> select cid, sum(price) from strans;
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'cid'
hive> select cid, sum(price) from strans group by cid;
Query ID = cloudera_20180716192929_b4c91210-a785-4794-ac96-a08eac6d023b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1531648645163_0006, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2018-07-16 19:29:58,126 Stage-1 map = 0%,  reduce = 0%
2018-07-16 19:30:18,036 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.99 sec
2018-07-16 19:30:37,416 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.48 sec
MapReduce Total cumulative CPU time: 3 seconds 480 msec
Ended Job = job_1531648645163_0006
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.48 sec   HDFS Read: 7716 HDFS Write: 29 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 480 msec
OK
c101    1130
c102    150
c103    1700
Time taken: 50.734 seconds, Fetched: 3 row(s)
hive>  create table prof2(name string, age int, wife struct<name:string,age:int , city:string>,city string)
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '#';
OK
Time taken: 0.152 seconds
hive> load data local inpath 'prof2' into table prof2;
Loading data to table practice.prof2
Table practice.prof2 stats: [numFiles=1, totalSize=50]
OK
Time taken: 0.273 seconds
hive> select * from prof2;
OK
Ravi    30      {"name":"Rani","age":25,"city":"del"}   hyd
Raghu   35      {"name":"Sailu","age":23,"city":"hyd"}  Del
Time taken: 0.075 seconds, Fetched: 2 row(s)
hive> select name, wife.name from prof2;
OK
Ravi    Rani
Raghu   Sailu
Time taken: 0.099 seconds, Fetched: 2 row(s)


[cloudera@quickstart ~]$ cat > prof2
Ravi,30,Rani#25#del,hyd
Raghu,35,Sailu#23#hyd,Del
^C
[cloudera@quickstart ~]$ cat prfo2
cat: prfo2: No such file or directory
[cloudera@quickstart ~]$ cat prof2
Ravi,30,Rani#25#del,hyd
Raghu,35,Sailu#23#hyd,Del
[cloudera@quickstart ~]$





name
====
abc
xyuz

length(name)-> 
3
4




Time taken: 0.055 seconds, Fetched: 2 row(s)
hive> create database practice1;
OK
Time taken: 0.119 seconds
hive> use practice1;
OK
Time taken: 0.043 seconds
hive> create table info1(a int, b int, c int);
OK
Time taken: 0.206 seconds
hive> load data local inpath 'file1' into table info1;
Loading data to table practice1.info1
Table practice1.info1 stats: [numFiles=1, totalSize=39]
OK
Time taken: 0.27 seconds
hive> select * from info1;
OK
NULL    NULL    NULL
NULL    NULL    NULL
NULL    NULL    NULL
NULL    NULL    NULL
Time taken: 0.113 seconds, Fetched: 4 row(s)
hive> create table emp(id int, name string, sal int, sex string, dno int)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.097 seconds
hive> load data local inpath 'emp1' into table emp;
Loading data to table practice1.emp
Table practice1.emp stats: [numFiles=1, totalSize=103]
OK
Time taken: 0.492 seconds
hive> select * from emp;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
Time taken: 0.086 seconds, Fetched: 5 row(s)
hive> create table empl(id int, name string, sal int, sex string, dno int)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.106 seconds
hive> load data local inpath 'emp1' into table empl;
Loading data to table practice1.empl
Table practice1.empl stats: [numFiles=1, totalSize=103]
OK
Time taken: 0.206 seconds
hive> load data local inpath 'emp2' into table empl;
Loading data to table practice1.empl
Table practice1.empl stats: [numFiles=2, totalSize=186]
OK
Time taken: 0.337 seconds
hive> load data local inpath 'emp2' into table emp;
Loading data to table practice1.emp
Table practice1.emp stats: [numFiles=2, totalSize=186]
OK
Time taken: 0.204 seconds
hive> select * from emp;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
201     kiran   14      m       90000
202     mani    12      f       10000
203     giri    12      m       20000
204     girija  11      f       40000
Time taken: 0.085 seconds, Fetched: 9 row(s)
hive> delete * from table emp where dno in (90000,10000);
Usage: delete [FILE|JAR|ARCHIVE] <value> [<value>]*
Query returned non-zero code: 1, cause: null
hive> select * from emp;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
201     kiran   14      m       90000
202     mani    12      f       10000
203     giri    12      m       20000
204     girija  11      f       40000
Time taken: 0.08 seconds, Fetched: 9 row(s)
hive> delete * from table emp where dno in (90000,10000,20000,40000);
Usage: delete [FILE|JAR|ARCHIVE] <value> [<value>]*
Query returned non-zero code: 1, cause: null
hive> load data local inpath 'emp2' into table emp;
Loading data to table practice1.emp
Table practice1.emp stats: [numFiles=3, totalSize=269]
OK
Time taken: 0.181 seconds
hive> select * from emp;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
201     kiran   14      m       90000
202     mani    12      f       10000
203     giri    12      m       20000
204     girija  11      f       40000
201     kiran   14      m       90000
202     mani    12      f       10000
203     giri    12      m       20000
204     girija  11      f       40000
Time taken: 0.067 seconds, Fetched: 13 row(s)
hive> drop table emp;
OK
Time taken: 1.705 seconds
hive> select * from emp;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'emp'
hive> create table emp(id int, name string, sal int, sex string, dno int)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.073 seconds
hive> truncate table empl;
OK
Time taken: 0.149 seconds
hive> select * from empl;
OK
Time taken: 0.071 seconds
hive> load data local inpath 'emp1' into table emp;
Loading data to table practice1.emp
Table practice1.emp stats: [numFiles=1, totalSize=103]
OK
Time taken: 0.19 seconds
hive> load data local inpath 'emp2' into table emp;
Loading data to table practice1.emp
Table practice1.emp stats: [numFiles=2, totalSize=164]
OK
Time taken: 0.21 seconds
hive> load data local inpath 'emp1' into table empl;
Loading data to table practice1.empl
Table practice1.empl stats: [numFiles=1, numRows=0, totalSize=103, rawDataSize=0]
OK
Time taken: 0.208 seconds
hive> load data local inpath 'emp2' into table empl;
Loading data to table practice1.empl
Table practice1.empl stats: [numFiles=2, numRows=0, totalSize=164, rawDataSize=0]
OK
Time taken: 0.212 seconds
hive> select * from emp;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
101     Venu    50000   m       12
102     Anu     40000   f       12
103     Bhoomi  30000   f       11
Time taken: 0.088 seconds, Fetched: 8 row(s)
hive> update emp set name="Hero" where name = "Venu"
    > ;
FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.
hive> create table epart(id int,name string,sal int,sex string,dno int)
    > partitioned by(s string);
OK
Time taken: 0.101 seconds
hive> insert overwrite table epart
    > partition(s='f')
    > select * from empl where sex='f';
Query ID = cloudera_20180716215858_94c80c9a-d795-417a-8222-c122a13051d6
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0008, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 21:58:09,732 Stage-1 map = 0%,  reduce = 0%
2018-07-16 21:58:18,425 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.58 sec
MapReduce Total cumulative CPU time: 1 seconds 580 msec
Ended Job = job_1531648645163_0008
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/epart/s=f/.hive-staging_hive_2018-07-16_21-58-00_755_793134302999049718-1/-ext-10000
Loading data to table practice1.epart partition (s=f)
Partition practice1.epart{s=f} stats: [numFiles=1, numRows=4, totalSize=83, rawDataSize=79]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.58 sec   HDFS Read: 4803 HDFS Write: 158 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 580 msec
OK
Time taken: 19.278 seconds
me taken: 18.52 seconds
hive> select * from epart where s = 'f';
OK
202     mani    10000   f       12      f
204     girija  40000   f       13      f
102     Anu     40000   f       12      f
103     Bhoomi  30000   f       11      f
Time taken: 0.637 seconds, Fetched: 4 row(s)
hive> select * from empl;
OK
201     kiran   90000   m       14
202     mani    10000   f       12
203     giri    20000   m       12
204     girija  40000   f       13
205     venu    50000   m       11
101     Venu    50000   m       12
102     Anu     40000   f       12
103     Bhoomi  30000   f       11
Time taken: 0.067 seconds, Fetched: 8 row(s)
hive> create table eparts(id int, name string,sal int, sex string, dno int)
    > partitioned by (d int)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.115 seconds
hive> insert overwrite table eparts
    > partition (d = 11)
    > select * from empl where dno =11;
Query ID = cloudera_20180716223535_5b0e0731-623e-42d3-a82d-0a18a4ba5f8e
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0010, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 22:35:41,355 Stage-1 map = 0%,  reduce = 0%
2018-07-16 22:35:50,163 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.66 sec
MapReduce Total cumulative CPU time: 1 seconds 660 msec
Ended Job = job_1531648645163_0010
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/eparts/d=11/.hive-staging_hive_2018-07-16_22-35-33_182_8255445011113141707-1/-ext-10000
Loading data to table practice1.eparts partition (d=11)
Partition practice1.eparts{d=11} stats: [numFiles=1, numRows=2, totalSize=42, rawDataSize=40]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.66 sec   HDFS Read: 4881 HDFS Write: 119 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 660 msec
OK
Time taken: 19.61 seconds
hive> insert overwrite table eparts
    > partition (d = 12)
    > select * from empl where dno =12;
Query ID = cloudera_20180716223636_da7fb48c-0d72-4a8d-8934-a47bca394a18
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0011, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 22:36:43,273 Stage-1 map = 0%,  reduce = 0%
2018-07-16 22:36:50,881 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.59 sec
MapReduce Total cumulative CPU time: 1 seconds 590 msec
Ended Job = job_1531648645163_0011
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/eparts/d=12/.hive-staging_hive_2018-07-16_22-36-35_162_6651812691502081404-1/-ext-10000
Loading data to table practice1.eparts partition (d=12)
Partition practice1.eparts{d=12} stats: [numFiles=1, numRows=4, totalSize=79, rawDataSize=75]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.59 sec   HDFS Read: 4881 HDFS Write: 156 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 590 msec
OK
Time taken: 17.105 seconds
hive> create table mpart(id int, name string, sal int, sex string, dno int)
    > partitioned by (d int, s string)
    > ;
OK
Time taken: 0.096 seconds
hive> insert overwrite table mpart
    > partition (d = 11 , s ='m')
    > select * from empl where dno=11 and sex = 'm';
Query ID = cloudera_20180716224444_aeb78f40-215f-4259-8f85-d60045d8a689
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0013, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0013
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 22:44:37,889 Stage-1 map = 0%,  reduce = 0%
2018-07-16 22:44:50,196 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1531648645163_0013
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/mpart/d=11/s=m/.hive-staging_hive_2018-07-16_22-44-28_664_5686729885856509117-1/-ext-10000
Loading data to table practice1.mpart partition (d=11, s=m)
Partition practice1.mpart{d=11, s=m} stats: [numFiles=1, numRows=1, totalSize=20, rawDataSize=19]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.92 sec   HDFS Read: 5177 HDFS Write: 100 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 920 msec
OK
Time taken: 22.936 seconds
hive> describe extended empl;
OK
id                      int                                       
name                    string                                    
sal                     int                                       
sex                     string                                    
dno                     int                                       

Detailed Table Information      Table(tableName:empl, dbName:practice1, owner:cloudera, createTime:1531802583, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:sal, type:int, comment:null), FieldSchema(name:sex, type:string, comment:null), FieldSchema(name:dno, type:int, comment:null)], location:hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/empl, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=2, transient_lastDdlTime=1531803109, COLUMN_STATS_ACCURATE=true, totalSize=164, numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
Time taken: 0.114 seconds, Fetched: 7 row(s)
hive>
hive> create table dpart(id int, name string,sal int, sex string, dno int)
    > partitioned by (d int, s string)
    > row format delimited fields terminated by ',';
OK
Time taken: 0.067 seconds
hive> set hive.exec.dynamic.partition=true
    > ;
hive> set hive.exec.dynamic.partition.mode=nonstrict;
hive> insert overwrite table dpart
    > partition (d,s)
    > select id,name,sal,sex,dno,dno,sex from empl;
Query ID = cloudera_20180716225656_220da499-4eb9-4fb5-a574-fd72266ee821
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0014, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0014
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-16 22:56:26,704 Stage-1 map = 0%,  reduce = 0%
2018-07-16 22:56:34,510 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec
MapReduce Total cumulative CPU time: 1 seconds 310 msec
Ended Job = job_1531648645163_0014
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/practice1.db/dpart/.hive-staging_hive_2018-07-16_22-56-17_209_8950179119979018888-1/-ext-10000
Loading data to table practice1.dpart partition (d=null, s=null)
         Time taken for load dynamic partitions : 588
        Loading partition {d=11, s=m}
        Loading partition {d=11, s=f}
        Loading partition {d=12, s=m}
        Loading partition {d=12, s=f}
        Loading partition {d=13, s=f}
        Loading partition {d=14, s=m}
         Time taken for adding to write entity : 2
Partition practice1.dpart{d=11, s=f} stats: [numFiles=1, numRows=1, totalSize=22, rawDataSize=21]
Partition practice1.dpart{d=11, s=m} stats: [numFiles=1, numRows=1, totalSize=20, rawDataSize=19]
Partition practice1.dpart{d=12, s=f} stats: [numFiles=1, numRows=2, totalSize=39, rawDataSize=37]
Partition practice1.dpart{d=12, s=m} stats: [numFiles=1, numRows=2, totalSize=40, rawDataSize=38]
Partition practice1.dpart{d=13, s=f} stats: [numFiles=1, numRows=1, totalSize=22, rawDataSize=21]
Partition practice1.dpart{d=14, s=m} stats: [numFiles=1, numRows=1, totalSize=21, rawDataSize=20]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.31 sec   HDFS Read: 4725 HDFS Write: 459 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 310 msec
OK
Time taken: 19.432 seconds
hive>




login as: cloudera
cloudera@192.168.216.129's password:
Last login: Tue Jul 17 19:56:58 2018 from 192.168.216.1
[cloudera@quickstart ~]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> create table raw(dt string, pr int)
    > row format delimited fields terminated by ',';
OK
Time taken: 0.719 seconds
hive> load data local inpath 'sales' into table raw;
Loading data to table default.raw
Table default.raw stats: [numFiles=1, totalSize=103]
OK
Time taken: 0.579 seconds
hive> create database hivedb;
OK
Time taken: 0.119 seconds
hive> use hivedb;
OK
Time taken: 0.025 seconds
hive> create table raw(dt string, pr int)
    > row format delimited fields terminated by ',';
OK
Time taken: 0.101 seconds
hive> load data local inpath 'sales' into table raw;
Loading data to table hivedb.raw
Table hivedb.raw stats: [numFiles=1, totalSize=103]
OK
Time taken: 0.2 seconds
hive> create table raw2(dt string, pr int);
OK
Time taken: 0.083 seconds
hive> insert into table raw2
    > select split(dt,'/'), amt from raw;
FAILED: SemanticException [Error 10004]: Line 2:22 Invalid table alias or column reference 'amt': (possible column names are: dt, pr)
hive> insert into table raw2
    > select split(dt,'/'), pr from raw;
Query ID = cloudera_20180717201111_bbf0d341-8c0c-44c4-a8fc-d939ef364fb5
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0015, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0015
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-17 20:11:29,134 Stage-1 map = 0%,  reduce = 0%
2018-07-17 20:11:37,179 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.56 sec
MapReduce Total cumulative CPU time: 1 seconds 560 msec
Ended Job = job_1531648645163_0015
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/hivedb.db/raw2/.hive-staging_hive_2018-07-17_20-11-18_886_5123365927974696516-1/-ext-10000
Loading data to table hivedb.raw2
Table hivedb.raw2 stats: [numFiles=1, numRows=7, totalSize=106, rawDataSize=99]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.56 sec   HDFS Read: 3983 HDFS Write: 173 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 560 msec
OK
Time taken: 20.933 seconds
hive> select concat(dt[2],"-",dt[0],"-",dt[1]), pr from raw2;
FAILED: SemanticException [Error 10033]: Line 1:14 [] not valid on non-collection types '2': string
hive> select * pr from raw;
FAILED: ParseException line 1:9 missing EOF at 'pr' near '*'
hive> select * from raw;
OK
01/01/2011      45000
01/01/2011      46000
01/23/2011      70000
02/01/2011      45000
03/01/2011      46000
03/23/2011      90000
        NULL
Time taken: 0.083 seconds, Fetched: 7 row(s)
hive> select * pr from raw2;
FAILED: ParseException line 1:9 missing EOF at 'pr' near '*'
hive> select * from raw2;
OK
01012011        45000
01012011        46000
01232011        70000
02012011        45000
03012011        46000
03232011        90000
        NULL
Time taken: 0.073 seconds, Fetched: 7 row(s)
hive> drop table raw2;
OK
Time taken: 0.192 seconds
hive> create table raw2(dt array<string>, pr int);
OK
Time taken: 0.108 seconds
hive> insert overwrite table raw2
    > select split(dt,'/'), pr from raw;
Query ID = cloudera_20180717202626_d8fe453f-e8bb-4493-ae51-e567ebbaa752
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0016, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0016
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-17 20:26:28,960 Stage-1 map = 0%,  reduce = 0%
2018-07-17 20:26:36,617 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec
MapReduce Total cumulative CPU time: 1 seconds 390 msec
Ended Job = job_1531648645163_0016
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/hivedb.db/raw2/.hive-staging_hive_2018-07-17_20-26-21_453_6288638947890628953-1/-ext-10000
Loading data to table hivedb.raw2
Table hivedb.raw2 stats: [numFiles=1, numRows=7, totalSize=106, rawDataSize=99]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.39 sec   HDFS Read: 4069 HDFS Write: 173 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 390 msec
OK
Time taken: 16.415 seconds
hive> select * from raw2;
OK
["01","01","2011"]      45000
["01","01","2011"]      46000
["01","23","2011"]      70000
["02","01","2011"]      45000
["03","01","2011"]      46000
["03","23","2011"]      90000
[]      NULL
Time taken: 0.077 seconds, Fetched: 7 row(s)
hive> select concat(dt[2],"-",dt[0],",",dt[1]),pr from raw2;
OK
2011-01,01      45000
2011-01,01      46000
2011-01,23      70000
2011-02,01      45000
2011-03,01      46000
2011-03,23      90000
NULL    NULL
Time taken: 0.09 seconds, Fetched: 7 row(s)
hive> select concat(dt[2],"-",dt[0],"-",dt[1]),pr from raw2;
OK
2011-01-01      45000
2011-01-01      46000
2011-01-23      70000
2011-02-01      45000
2011-03-01      46000
2011-03-23      90000
NULL    NULL
Time taken: 0.083 seconds, Fetched: 7 row(s)
hive> select concat(dt[2],"-",dt[0],'-',dt[1]),pr from raw2;
OK
2011-01-01      45000
2011-01-01      46000
2011-01-23      70000
2011-02-01      45000
2011-03-01      46000
2011-03-23      90000
NULL    NULL
Time taken: 0.076 seconds, Fetched: 7 row(s)
hive> create table rawx like raw;
OK
Time taken: 0.106 seconds
hive> select * from rawx;
OK
Time taken: 0.063 seconds
hive> insert into table rawx
    > select dt,pr from raw
    > union all
    > select concat(substr(dt,1,9),'2') as dt,
    > amt+2000 as pr
    > union all
    > select concat(substr(dt,1,9),'3') as dt,
    > amt+10000 as pr;
FAILED: SemanticException [Error 10004]: Line 4:21 Invalid table alias or column reference 'dt': (possible column names are: )
hive> select * from rawx;
OK
Time taken: 0.068 seconds
hive> create table sales(dt string, amt int);
OK
Time taken: 0.095 seconds
hive> insert into table sales
    > select concat(dt[2],'-',dt[0],'-',dt[1])
    > amt from raw2;
FAILED: SemanticException [Error 10044]: Line 1:18 Cannot insert into target table because column number/types are different 'sales': Table insclause-0 has 2 columns, but query has 1 columns.
hive> insert into table sales
    > select concat(dt[2],'-',dt[0],'-',dt[1]),
    > pr from raw2;
Query ID = cloudera_20180717204646_5942fb52-088e-48a4-95a0-cb37647d2949
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0017, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0017
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-17 20:46:43,556 Stage-1 map = 0%,  reduce = 0%
2018-07-17 20:46:52,327 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.58 sec
MapReduce Total cumulative CPU time: 1 seconds 580 msec
Ended Job = job_1531648645163_0017
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/hivedb.db/sales/.hive-staging_hive_2018-07-17_20-46-35_292_375085086154244756-1/-ext-10000
Loading data to table hivedb.sales
Table hivedb.sales stats: [numFiles=1, numRows=7, totalSize=108, rawDataSize=101]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.58 sec   HDFS Read: 4192 HDFS Write: 177 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 580 msec
OK
Time taken: 18.331 seconds
hive> select * from sales limit 2
    > ;
OK
2011-01-01      45000
2011-01-01      46000
Time taken: 0.074 seconds, Fetched: 2 row(s)
hive> create table spart(dt string,amt int)
    > partitioned by (y int, m int, d int)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.092 seconds
hive> set hive.exec.partition.dyamic = true
    > ;
hive> set hive.exec.partition.dyamic.mode = nonstrict;
hive> set hive.exec.partition.dyamic  = true
    > ;
hive> insert overwrite table spart
    > partition(y , m , d)
    > select dt, amt , year(dt),month(dt),day(dt) from sales;
FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict
hive> set hive.exec.dynamic.partition.mode = nonstrict
    > ;
hive> insert overwrite table spart
    > partition(y , m , d)
    > select dt, amt , year(dt),month(dt),day(dt) from sales;
Query ID = cloudera_20180717212323_d15d8942-eaed-4287-97d2-acf5229b2880
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1531648645163_0018, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1531648645163_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1531648645163_0018
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-07-17 21:23:51,760 Stage-1 map = 0%,  reduce = 0%
2018-07-17 21:23:59,345 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.67 sec
MapReduce Total cumulative CPU time: 1 seconds 670 msec
Ended Job = job_1531648645163_0018
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/hivedb.db/spart/.hive-staging_hive_2018-07-17_21-23-44_017_7364145505943276627-1/-ext-10000
Loading data to table hivedb.spart partition (y=null, m=null, d=null)
         Time taken for load dynamic partitions : 589
        Loading partition {y=2011, m=2, d=1}
        Loading partition {y=2011, m=1, d=1}
        Loading partition {y=2011, m=1, d=23}
        Loading partition {y=__HIVE_DEFAULT_PARTITION__, m=__HIVE_DEFAULT_PARTITION__, d=__HIVE_DEFAULT_PARTITION__}
        Loading partition {y=2011, m=3, d=1}
        Loading partition {y=2011, m=3, d=23}
         Time taken for adding to write entity : 0
Partition hivedb.spart{y=2011, m=1, d=1} stats: [numFiles=1, numRows=2, totalSize=34, rawDataSize=32]
Partition hivedb.spart{y=2011, m=1, d=23} stats: [numFiles=1, numRows=1, totalSize=17, rawDataSize=16]
Partition hivedb.spart{y=2011, m=2, d=1} stats: [numFiles=1, numRows=1, totalSize=17, rawDataSize=16]
Partition hivedb.spart{y=2011, m=3, d=1} stats: [numFiles=1, numRows=1, totalSize=17, rawDataSize=16]
Partition hivedb.spart{y=2011, m=3, d=23} stats: [numFiles=1, numRows=1, totalSize=17, rawDataSize=16]
Partition hivedb.spart{y=__HIVEFAULT_PARTITION__, d=__HIVE_DEFAULT_PARTITION__} stats: [numFiles=1, numRows=1, totalSize=6, rawDataSize=5]
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 1.67 sec   HDFS Read: 4512 HDFS Write: 497 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 670 msec
OK
Time taken: 17.583 seconds
hive> select * from spart;
OK
2011-01-01      45000   2011    1       1
2011-01-01      46000   2011    1       1
2011-01-23      70000   2011    1       23
2011-02-01      45000   2011    2       1
2011-03-01      46000   2011    3       1
2011-03-23      90000   2011    3       23
NULL    NULL    NULL    NULL    NULL
Time taken: 0.182 seconds, Fetched: 7 row(s)
hive> select * frmo sales;
FAILED: ParseException line 1:9 missing EOF at 'frmo' near '*'
hive> select * from sales;
OK
2011-01-01      45000
2011-01-01      46000
2011-01-23      70000
2011-02-01      45000
2011-03-01      46000
2011-03-23      90000
NULL    NULL
Time taken: 0.106 seconds, Fetched: 7 row(s)
hive>


==========================================================================================



Bucketing:

which divides the table data into multiple data files

each bucket is a data file

adv: sampling technique

p1,p2,p3,p1,p2,p3,p4

buck1	buck2

buck1 --- p1,p3,p1,p3
buck2 --- p2,p2,p4

2 buckets as b1,b2

p1-> 100%2  --> 0 -> b1
p2-> 101%2  ->  1-> b2

single bucket can have miultiple keys but a key is available with only one bucket

transactions
p1,1000
p2,9999
p1,1200
p3,2300
p1,1300
p2,6000


create table trans(pid string, amt int)
row format delimited 
fields terminated by ',';

load data local inpath 'transactions' into table trans;

set hive.enforce.bucketing = true;

create table bucks (pid string, amt int)
clustered by (pid)
into 3 buckets;

insert overwrite table bucks 


Bucketing will be done after partitioning
partitioned by may have nested columns with diff column names but clustered by should have  column name and it should exist in the table

IMPORTANTTTTTTTT

Bucketing maily avoids IO costs by removing SHUFFLE and SORT operations

normal join query

Table scan
Shuffle(exchange)
Sort
Sort Merge join

Bucketed tables join

Sort Merge Join
Table Scan



Hive Bucketing

Shuffle and sort is costly due to disk and network IO
Bucketing will prshuffle and sort the input
tables used frequently in JOINS with same key should use bucketing
Loading of data in cumulative tables


Uses MR so Reducer receives shuffled and sorted rows so reads are faster

Spark doesnt use Reducers 

So it should shuffle and sort for read, so reads are costly

Spark uses Murmu3Hash for bucketing

Hive uses inbuilt hash

  




================================================

SERDE concept in Hive:

Serialization: Converting an object into binary format
Deser: opp to serialization

add jar /home/training/desktop/hive-json-serde-0.2.jar

create table json_serde(a int, b int)
row format serde 'org.apache.hadoop.hive.serde2.JsonSerde';

load data local inpath 'json2' into table json_serde;

==========================================================================

Dealing with multidelimited data:

1)create a single column table 'rawdata'
2)load raw data into table
3)create a table cleandata(id int, name string)
4) insert overwrite table cleandata select substr(col1,1,3),substr(col1,5,3) from raw_data

substr- starts from 1, length of substring is 3(1st to 3fields)

================================================

Scripting in Hive:

gedit hivesc.hql

in script:

set hive.cli.print.header=true;

many queries.....

hive -f hivesc  //to run script

In prod, we only use scripts

if there is any error in script, script will be terminated abnormally

main adv: easily edit queries and easil run multiple queries
===========================================

Regular Expressions in Hive:

[]  --in between
[^] -- not in between

create table tb_name(name string, age int)
row format delimited
fiels terminated by '\t'
lines terminated by '\n'
stored as textfile;


create external table raw_web_logs(
ip string,
date_of string,
method string,
url string,
http_version string,
code1 string, 
code2 string,
dash string,
user_agent string)
row format serde 'org.apache.hadoop.hive.contrib.serde2.regexserde'
with serdeproperties(
'input.regex'=)

=================================================


Nested XML in Hive:
same 4 step strategy

1)  create a single column table
2)  load raw data into table
3) create a required columns table 
4) insert overwrite table2 select xpath_string(col1, 'rec/properties/age'), xpath_string(col1,'rec/name/age') from table1;

=========================================================
Nested JSON in Hive:

1) create single column table
2) load data
3) create structured table
4) Insert overwrite table2 select get_json_object(col1, '$.name'), get_json_object(col1, '$.details')
from table1

5) create table table3(id,name,a int, b int, loc)  //here a and b will be from details

6) insert overwrite table3 select id,name,get_json_object(details,'$.a'),get_json_object(details,'$.b')
from table2

==========================================================
URL data(parse_url):

Log files are divided into 3phases

Host: google.com
path: drive or account or login
query: userid=123&pwd=***....


4 steps strategy

1)create single column table
2) load data
3) create structured(reqd columns) table
4) insert overwrite table table2
	select parse_url(col1,"HOST"),parse_url(col1,"PATH"),parse_url(col1,"QUERY")

next we have to convert QUERY (it has username and pwd) to json 

create table table3(gmap MAP<string,string>);
insert overwrite table3 
select str_to_map(qry,'&','=') from tab2


connect beeline:

beeline -u jdbc:hive2://localhost:10000

alter table t rename to t1

alter table t1 add columns(col3 int)

alter table t1 change col1 col11 string after col3   //to change the column name or datatype name   
usually altered column will be at the end of all columns //except partitined column

alter table t1 set location '/user/cloudera/hive'

alter table t1 set fileformat orc

alter table t1 set TBLPROPERTIES()

